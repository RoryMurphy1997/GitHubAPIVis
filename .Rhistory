#Since eggs has no seasonality, seasonplot and plot(decompose) do not work. Since plot and acf are covered in
#tsdisplay, I decided to graph with it alone.
tsdisplay(eggs)
#Based on the plot, the egg time series has a trend component (the gradual dip in the ACF affirms this), some noise
#element and no seasonal component. Thus, a seasonplot cannot be made from this time series. From this, either the
#Simple Exponential Smoothing (SES) or the Double Exponential Smoothing (DES) Holt-Winters algorithm will be the best
#choice for forecasting with this time series.
#SES
HoltWinters(eggs, beta=FALSE, gamma = FALSE)
require(forecast)
require(fma)
#Since eggs has no seasonality, seasonplot and plot(decompose) do not work. Since plot and acf are covered in
#tsdisplay, I decided to graph with it alone.
tsdisplay(eggs)
#Based on the plot, the egg time series has a trend component (the gradual dip in the ACF affirms this), some noise
#element and no seasonal component. Thus, a seasonplot cannot be made from this time series. From this, either the
#Simple Exponential Smoothing (SES) or the Double Exponential Smoothing (DES) Holt-Winters algorithm will be the best
#choice for forecasting with this time series.
#SES
HoltWinters(eggs, beta=FALSE, gamma = FALSE)
HoltWinters(eggs, beta=FALSE, gamma = FALSE)$SSE
#SSE = 66362.79
#alpha = 62.86069
#DES
HoltWinters(eggs, gamma = FALSE)
require(forecast)
require(fma)
#Since eggs has no seasonality, seasonplot and plot(decompose) do not work. Since plot and acf are covered in
#tsdisplay, I decided to graph with it alone.
tsdisplay(eggs)
#Based on the plot, the egg time series has a trend component (the gradual dip in the ACF affirms this), some noise
#element and no seasonal component. Thus, a seasonplot cannot be made from this time series. From this, either the
#Simple Exponential Smoothing (SES) or the Double Exponential Smoothing (DES) Holt-Winters algorithm will be the best
#choice for forecasting with this time series.
#SES
HoltWinters(eggs, beta=FALSE, gamma = FALSE)
HoltWinters(eggs, beta=FALSE, gamma = FALSE)$SSE
require(forecast)
require(fma)
#Since eggs has no seasonality, seasonplot and plot(decompose) do not work. Since plot and acf are covered in
#tsdisplay, I decided to graph with it alone.
tsdisplay(eggs)
#Based on the plot, the egg time series has a trend component (the gradual dip in the ACF affirms this), some noise
#element and no seasonal component. Thus, a seasonplot cannot be made from this time series. From this, either the
#Simple Exponential Smoothing (SES) or the Double Exponential Smoothing (DES) Holt-Winters algorithm will be the best
#choice for forecasting with this time series.
#SES
HoltWinters(eggs, beta=FALSE, gamma = FALSE)
HoltWinters(eggs, beta=FALSE, gamma = FALSE)$SSE
#SSE = 66362.79
#alpha = 0.8564241
#a = parameter for forecasting next value
#DES
HoltWinters(eggs, gamma = FALSE)
HoltWinters(eggs, gamma = FALSE)$SSE
iris
require('MASS')
require('class')
iris
dim(iris)
index = c(1:30, 51:80, 101:130)
train=iris[index,1:4]
#Items not in index go to test (601:900,1500:1800) with 300 each from simA and simB
test=iris[-index,1:4]
result=knn(train, test, cl=iris[index,5], k=3)
result
require('MASS')
require('class')
dim(iris)
index = c(1:30, 51:80, 101:130)
train=iris[index,1:4]
#Items not in index go to test (601:900,1500:1800) with 300 each from simA and simB
test=iris[-index,1:4]
result=knn(train, test, cl=iris[index,5], k=3)
result
(nrow(test)-sum(diag(table(result, simT[-index,3]))))/nrow(test)
require('MASS')
require('class')
dim(iris)
index = c(1:30, 51:80, 101:130)
train=iris[index,1:4]
#Items not in index go to test (601:900,1500:1800) with 300 each from simA and simB
test=iris[-index,1:4]
result=knn(train, test, cl=iris[index,5], k=3)
result
(nrow(test)-sum(diag(table(result, iris[-index,5]))))/nrow(test)
require('MASS')
require('class')
dim(iris)
index = c(1:30, 51:80, 101:130)
train=iris[index,1:4]
#Items not in index go to test (601:900,1500:1800) with 300 each from simA and simB
test=iris[-index,1:4]
result=knn(train, test, cl=iris[index,5], k=3)
result
(nrow(test)-sum(diag(table(result, iris[-index,5]))))/nrow(test)
k=c(1:10)
p=rep(0,10)
summary=cbind(k,p)
colnames(summary)=c("k","Percent Misclassification")
for(i in 1:10)
{
result=knn(train, test, cl=simT[index,3], k=i)
summary[i,2]=(nrow(test) - sum(diag(table(result, simT[-index,3]))))/nrow(test)
}
summary
plot(summary, type="l")
require('MASS')
require('class')
dim(iris)
index = c(1:30, 51:80, 101:130)
train=iris[index,1:4]
#Items not in index go to test (601:900,1500:1800) with 300 each from simA and simB
test=iris[-index,1:4]
result=knn(train, test, cl=iris[index,5], k=3)
result
(nrow(test)-sum(diag(table(result, iris[-index,5]))))/nrow(test)
k=c(1:10)
p=rep(0,10)
summary=cbind(k,p)
colnames(summary)=c("k","Percent Misclassification")
for(i in 1:10)
{
result=knn(train, test, cl=iris[index,5], k=i)
summary[i,2]=(nrow(test) - sum(diag(table(result, iris[-index,5]))))/nrow(test)
}
summary
plot(summary, type="l")
?predict
protein=read.table("protein.txt", header = T)
protein
proteinNumeric = protein[,2:10]
var(proteinNumeric)
PCA = prcomp(proteinNumeric, scale = TRUE)
PCA
summary(PCA)
protein=read.table("protein.txt", header = T)
protein
proteinNumeric = protein[,2:10]
var(proteinNumeric)
PCA = prcomp(proteinNumeric, scale = TRUE)
PCA
summary(PCA)
plot(PCA, type='l')
protein=read.table("protein.txt", header = T)
protein
proteinNumeric = protein[,2:10]
var(proteinNumeric)
PCA = prcomp(proteinNumeric, scale = TRUE)
PCA
summary(PCA)
plot(PCA, type='l')
newProtein = predict(PCA)
newProtein
protein=read.table("protein.txt", header = T)
protein
proteinNumeric = protein[,2:10]
var(proteinNumeric)
PCA = prcomp(proteinNumeric, scale = TRUE)
PCA
summary(PCA)
plot(PCA, type='l')
newProtein = predict(PCA)
newProtein
protein
protein=read.table("protein.txt", header = T)
protein
proteinNumeric = protein[,2:10]
var(proteinNumeric)
PCA = prcomp(proteinNumeric, scale = TRUE)
PCA
summary(PCA)
plot(PCA, type='l')
newProtein = predict(PCA)
newProtein
protein
?plot
plot(newProtein[,2], newProtein[,3], type="n", xlab="PC1", ylab="PC2")
text(newProtein[,2], newProtein[,3], labels=substr(protein[,1],1,2), col=as.integer(protein[,1]))
protein=read.table("protein.txt", header = T)
protein
proteinNumeric = protein[,2:10]
var(proteinNumeric)
PCA = prcomp(proteinNumeric, scale = TRUE)
PCA
summary(PCA)
plot(PCA, type='l')
newProtein = predict(PCA)
newProtein
cbind(newProtein, protein[,1])
newProtein
protein=read.table("protein.txt", header = T)
protein
proteinNumeric = protein[,2:10]
var(proteinNumeric)
PCA = prcomp(proteinNumeric, scale = TRUE)
PCA
summary(PCA)
plot(PCA, type='l')
newProtein = predict(PCA)
newProtein
newProtein= cbind(newProtein, protein[,2])
newProtein
protein=read.table("protein.txt", header = T)
protein
proteinNumeric = protein[,2:10]
var(proteinNumeric)
PCA = prcomp(proteinNumeric, scale = TRUE)
PCA
summary(PCA)
plot(PCA, type='l')
newProtein = predict(PCA)
newProtein
newProtein= cbind(newProtein, protein[,1])
protein=read.table("protein.txt", header = T)
protein
proteinNumeric = protein[,2:10]
var(proteinNumeric)
PCA = prcomp(proteinNumeric, scale = TRUE)
PCA
summary(PCA)
plot(PCA, type='l')
newProtein = predict(PCA)
newProtein
newProtein= cbind(newProtein, protein[,1])
newProtein
#Iris Data
iris
length(iris)
width(iris)
height(iris)
?iris
#Iris Data
which(iris[,5] == setosa)
#Iris Data
which(iris[,5] == 'setosa')
#Iris Data
which(iris[,5] == 'versicolor')
#Iris Data
which(iris[,5] == 'virginica')
itrain = c(iris[1:30,],iris[51:80,],iris[101:130,])
itest = c(iris[31:40,],iris[81:90,],iris[131:140,])
ivalidate = c(iris[41:50,],iris[91:100,],iris[141:150,])
lda = lda(itrain[,-5], grouping = itrain[,5])
qda = qda(itrain[,-5], grouping = itrain[,5])
predict(lda,itest[,-5])
predict(qda,itest[,-5])
itrain = c(iris[1:30,],iris[51:80,],iris[101:130,])
itest = c(iris[31:40,],iris[81:90,],iris[131:140,])
ivalidate = c(iris[41:50,],iris[91:100,],iris[141:150,])
ilda = lda(itrain[,-5], grouping = itrain[,5])
iqda = qda(itrain[,-5], grouping = itrain[,5])
predict(ilda,itest[,-5])
predict(iqda,itest[,-5])
itrain = c(iris[1:30,],iris[51:80,],iris[101:130,])
itest = c(iris[31:40,],iris[81:90,],iris[131:140,])
ivalidate = c(iris[41:50,],iris[91:100,],iris[141:150,])
ilda = lda(itrain[,-5], grouping = itrain[,5])
iqda = qda(itrain[,-5], grouping = itrain[,5])
predict(ilda,itest[,-5])
predict(iqda,itest[,-5])
require('MASS')
itrain = c(iris[1:30,],iris[51:80,],iris[101:130,])
itest = c(iris[31:40,],iris[81:90,],iris[131:140,])
ivalidate = c(iris[41:50,],iris[91:100,],iris[141:150,])
ilda = lda(itrain[,-5], grouping = itrain[,5])
iqda = qda(itrain[,-5], grouping = itrain[,5])
predict(ilda,itest[,-5])
predict(iqda,itest[,-5])
itrain = c(iris[1:30,],iris[51:80,],iris[101:130,])
itest = c(iris[31:40,],iris[81:90,],iris[131:140,])
ivalidate = c(iris[41:50,],iris[91:100,],iris[141:150,])
ilda = lda(itrain[,-5], grouping = itrain[,5])
iqda = qda(itrain[,-5], grouping = itrain[,5])
predict(ilda,itest[,-5])
predict(iqda,itest[,-5])
itrain = c(iris[1:30,],iris[51:80,],iris[101:130,])
itest = c(iris[31:40,],iris[81:90,],iris[131:140,])
ivalidate = c(iris[41:50,],iris[91:100,],iris[141:150,])
ilda = lda(itrain[,c(1,2,3,4)], grouping = itrain[,5])
iqda = qda(itrain[,c(1,2,3,4)], grouping = itrain[,5])
predict(ilda,itest[,-5])
predict(iqda,itest[,-5])
?lda
itrain = c(iris[1:30,],iris[51:80,],iris[101:130,])
itest = c(iris[31:40,],iris[81:90,],iris[131:140,])
ivalidate = c(iris[41:50,],iris[91:100,],iris[141:150,])
ilda = lda(itrain[,c(1,2,3,4)], grouping = itrain[,5])
iqda = qda(itrain[,c(1,2,3,4)], grouping = itrain[,5])
predict(ilda,itest[,-5])
predict(iqda,itest[,-5])
itrain = c(iris[1:30,],iris[51:80,],iris[101:130,])
itrain
itrain = iris[c(1:30,51:80,101:130),]
itrain
itrain = iris[c(1:30,51:80,101:130),]
itest = iris[c(31:40,81:90,131:140),]
ivalidate = iris[c(41:50,91:100,141:150),]
ilda = lda(itrain[,c(1,2,3,4)], grouping = itrain[,5])
iqda = qda(itrain[,c(1,2,3,4)], grouping = itrain[,5])
predict(ilda,itest[,-5])
predict(iqda,itest[,-5])
require('MASS')
itrain = iris[c(1:30,51:80,101:130),]
itest = iris[c(31:40,81:90,131:140),]
ivalidate = iris[c(41:50,91:100,141:150),]
ilda = lda(itrain[,c(1,2,3,4)], grouping = itrain[,5])
iqda = qda(itrain[,c(1,2,3,4)], grouping = itrain[,5])
predict(ilda,itest[,-5])
predict(iqda,itest[,-5])
itrain = iris[c(1:30,51:80,101:130),]
itest = iris[c(31:40,81:90,131:140),]
ivalidate = iris[c(41:50,91:100,141:150),]
ilda = lda(itrain[,c(1,2,3,4)], grouping = itrain[,5])
iqda = qda(itrain[,c(1,2,3,4)], grouping = itrain[,5])
predict(ilda,itest[,-5])
predict(iqda,itest[,-5])
#LDA:
#QDA:
predict(ilda,ivalidate[,-5])
predict(iqda,ivalidate[,-5])
predict(ilda,ivalidate[,-5])
predict(iqda,ivalidate[,-5])
ivalidate
predict(ilda,itest[,-5])
predict(iqda,itest[,-5])
itest
?sd
?randIndex
require('modeltools')
require('flexclust')
?randIndex
prcomp(iris[,1:4],scale=TRUE)
summary(prcomp(iris[,1:4],scale=TRUE))
#2: prcomp
cov(iris[,1:4])
?isoMDS
#Kruskal's Non-Metric scaling
require("MASS")
?isoMDS
cl2=kmeans(faithful, centers = k)
#Check further
k=2
cl2=kmeans(faithful, centers = k)
#Alternatively
cl2$size
#Cluster centres
cl2$centers
summary(cl2)
#Cluster 1 has smaller eruptions and weighting times while cluster 2 has  larger
#Withing Sum of Squares
cl2$withinss
#Distance between cluster centers
dist(cl2$centers,method = "euclidian")
dist(cl10$centers, method = "euclidian")
load("olive.Rdata")
SSOlive = rep(0,10)
SSOlive[1] = (n-1)*sum(apply(olive,2,var))
for(i in 2:10)
{
SSOlive[i] = sum(kmeans(olive, centers=i)$withinss)
}
SSOlive
#k=10
cl10 = kmeans(olive, centers = 10)
ave = rep(0,10)
for(i in 1:10)
{
g1=olive[which(cl10$cluster==i),] #Selects datain first cluster
ng1=cl10$size[i] #records no of data points in cluster 1
total1=sum(as.matrix(dist(rbind(g1,cl10$centers[i,])))[ng1+1]) #Adds cluster center as additional row of cluster data, creates a distance matrix and sums them
ave[i]=total1 / ng1
}
ave
dist(cl10$centers, method = "euclidian")
setwd("C:/Users/Cormac/Downloads")
#Olive Oil Data
load("olive.Rdata")
SSOlive = rep(0,10)
SSOlive[1] = (n-1)*sum(apply(olive,2,var))
for(i in 2:10)
{
SSOlive[i] = sum(kmeans(olive, centers=i)$withinss)
}
SSOlive
#k=10
cl10 = kmeans(olive, centers = 10)
ave = rep(0,10)
for(i in 1:10)
{
g1=olive[which(cl10$cluster==i),] #Selects datain first cluster
ng1=cl10$size[i] #records no of data points in cluster 1
total1=sum(as.matrix(dist(rbind(g1,cl10$centers[i,])))[ng1+1]) #Adds cluster center as additional row of cluster data, creates a distance matrix and sums them
ave[i]=total1 / ng1
}
ave
dist(cl10$centers, method = "euclidian")
?sammon
require("MASS")
?sammon
shiny::runApp('~/GitHub/GitHubAPIVis')
source('~/GitHub/GitHubAPIVis/Test.R')
install.packages("shiny")
library(shiny); source('~/GitHub/GitHubAPIVis/Test.R')
source('~/GitHub/GitHubAPIVis/Test.R')
install.packages("shiny")
source('~/GitHub/GitHubAPIVis/Test.R')
source('~/GitHub/GitHubAPIVis/Test.R')
runApp('~/GitHub/GitHubAPIVis')
runApp('~/GitHub/GitHubAPIVis')
timeSeries = read.csv("P3timeseriesStudent.csv", header=FALSE)
timeSeries
tsdisplay(timeSeries,lag.max = 100)
timeSeriesSeasonal = ts(timeSeries[49:100,], frequency = 12)
ggseasonplot(timeSeriesSeasonal)
timeSeriesSeasonal
tsdisplay(timeSeriesSeasonal, lag.max = 50)
#d=1
tsdisplay(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(0,0,0))$residuals, lag.max=50)
Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(0,0,0))
#D=1
tsdisplay(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(0,1,0))$residuals, lag.max=50)
Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(0,1,0))
#P=1
tsdisplay(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(1,1,0))$residuals, lag.max=50)
Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(1,1,0))
#AIC = 192.55 BIC = 195.82
plot(forecast(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(1,1,0)), h=20))
forecast(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(1,1,0)), h=20)
require(forecast)
require(fma)
timeSeries = read.csv("P3timeseriesStudent.csv", header=FALSE)
timeSeries
tsdisplay(timeSeries,lag.max = 100)
timeSeriesSeasonal = ts(timeSeries[49:100,], frequency = 12)
ggseasonplot(timeSeriesSeasonal)
timeSeriesSeasonal
tsdisplay(timeSeriesSeasonal, lag.max = 50)
#d=1
tsdisplay(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(0,0,0))$residuals, lag.max=50)
Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(0,0,0))
#D=1
tsdisplay(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(0,1,0))$residuals, lag.max=50)
Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(0,1,0))
#P=1
tsdisplay(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(1,1,0))$residuals, lag.max=50)
Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(1,1,0))
#AIC = 192.55 BIC = 195.82
plot(forecast(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(1,1,0)), h=20))
forecast(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(1,1,0)), h=20)
#d=1
tsdisplay(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(0,0,0))$residuals, lag.max=50)
#d=1
tsdisplay(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(0,0,0))$residuals, lag.max=50)
par(mar=c(11,1,5,1))
#d=1
tsdisplay(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(0,0,0))$residuals, lag.max=50)
par(mar=c(11,1,1,1))
#d=1
tsdisplay(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(0,0,0))$residuals, lag.max=50)
par(mar=c(1,1,1,1))
#d=1
tsdisplay(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(0,0,0))$residuals, lag.max=50)
par(mar=c(1,2,3,1))
#d=1
tsdisplay(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(0,0,0))$residuals, lag.max=50)
#D=1
tsdisplay(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(0,1,0))$residuals, lag.max=50)
plot(forecast(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(1,1,0)), h=20))
forecast(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(1,1,0)), h=20)
timeSeries = read.csv("P3timeseriesStudent.csv", header=FALSE)
tsdisplay(timeSeries,lag.max = 100)
timeSeriesSeasonal = ts(timeSeries[50:100,], frequency = 12)
ggseasonplot(timeSeriesSeasonal)
tsdisplay(timeSeriesSeasonal, lag.max = 50)
#d=1
tsdisplay(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(0,0,0))$residuals, lag.max=50)
Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(0,0,0))
#D=1
tsdisplay(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(0,1,0))$residuals, lag.max=50)
Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(0,1,0))
#P=1
tsdisplay(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(1,1,0))$residuals, lag.max=50)
Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(1,1,0))
#AIC = 192.55 BIC = 195.82
plot(forecast(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(1,1,0)), h=20))
forecast(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(1,1,0)), h=20)
plot(forecast(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(1,1,0)), h=18))
forecast(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(1,1,0)), h=18)
runApp('~/GitHub/GitHubAPIVis')
runApp('~/GitHub/GitHubAPIVis')
runApp('~/GitHub/GitHubAPIVis')
runApp('~/GitHub/GitHubAPIVis')
visFullData
order(visFullData$numberOfAppearances)
?order
visFullData[order(visFullData$numberOfAppearances),]
visFullData[order(visFullData$numberOfAppearances, decreasing = FALSE),]
runApp('~/GitHub/GitHubAPIVis')
runApp('~/GitHub/GitHubAPIVis')
runApp('~/GitHub/GitHubAPIVis')
runApp('~/GitHub/GitHubAPIVis')
runApp('~/GitHub/GitHubAPIVis')
runApp('~/GitHub/GitHubAPIVis')
runApp('~/GitHub/GitHubAPIVis')
runApp("app.R")
setwd("~/GitHub/GitHubAPIVis")
runApp("app.R")
#install.packages("shiny")
#library(shiny)
#runApp("app.R")
runGitHub( "GitHubAPIVis", "RoryMurphy1997")
