summary(PCA)
protein=read.table("protein.txt", header = T)
protein
proteinNumeric = protein[,2:10]
var(proteinNumeric)
PCA = prcomp(proteinNumeric, scale = TRUE)
PCA
summary(PCA)
plot(PCA, type='l')
protein=read.table("protein.txt", header = T)
protein
proteinNumeric = protein[,2:10]
var(proteinNumeric)
PCA = prcomp(proteinNumeric, scale = TRUE)
PCA
summary(PCA)
plot(PCA, type='l')
newProtein = predict(PCA)
newProtein
protein=read.table("protein.txt", header = T)
protein
proteinNumeric = protein[,2:10]
var(proteinNumeric)
PCA = prcomp(proteinNumeric, scale = TRUE)
PCA
summary(PCA)
plot(PCA, type='l')
newProtein = predict(PCA)
newProtein
protein
protein=read.table("protein.txt", header = T)
protein
proteinNumeric = protein[,2:10]
var(proteinNumeric)
PCA = prcomp(proteinNumeric, scale = TRUE)
PCA
summary(PCA)
plot(PCA, type='l')
newProtein = predict(PCA)
newProtein
protein
?plot
plot(newProtein[,2], newProtein[,3], type="n", xlab="PC1", ylab="PC2")
text(newProtein[,2], newProtein[,3], labels=substr(protein[,1],1,2), col=as.integer(protein[,1]))
protein=read.table("protein.txt", header = T)
protein
proteinNumeric = protein[,2:10]
var(proteinNumeric)
PCA = prcomp(proteinNumeric, scale = TRUE)
PCA
summary(PCA)
plot(PCA, type='l')
newProtein = predict(PCA)
newProtein
cbind(newProtein, protein[,1])
newProtein
protein=read.table("protein.txt", header = T)
protein
proteinNumeric = protein[,2:10]
var(proteinNumeric)
PCA = prcomp(proteinNumeric, scale = TRUE)
PCA
summary(PCA)
plot(PCA, type='l')
newProtein = predict(PCA)
newProtein
newProtein= cbind(newProtein, protein[,2])
newProtein
protein=read.table("protein.txt", header = T)
protein
proteinNumeric = protein[,2:10]
var(proteinNumeric)
PCA = prcomp(proteinNumeric, scale = TRUE)
PCA
summary(PCA)
plot(PCA, type='l')
newProtein = predict(PCA)
newProtein
newProtein= cbind(newProtein, protein[,1])
protein=read.table("protein.txt", header = T)
protein
proteinNumeric = protein[,2:10]
var(proteinNumeric)
PCA = prcomp(proteinNumeric, scale = TRUE)
PCA
summary(PCA)
plot(PCA, type='l')
newProtein = predict(PCA)
newProtein
newProtein= cbind(newProtein, protein[,1])
newProtein
#Iris Data
iris
length(iris)
width(iris)
height(iris)
?iris
#Iris Data
which(iris[,5] == setosa)
#Iris Data
which(iris[,5] == 'setosa')
#Iris Data
which(iris[,5] == 'versicolor')
#Iris Data
which(iris[,5] == 'virginica')
itrain = c(iris[1:30,],iris[51:80,],iris[101:130,])
itest = c(iris[31:40,],iris[81:90,],iris[131:140,])
ivalidate = c(iris[41:50,],iris[91:100,],iris[141:150,])
lda = lda(itrain[,-5], grouping = itrain[,5])
qda = qda(itrain[,-5], grouping = itrain[,5])
predict(lda,itest[,-5])
predict(qda,itest[,-5])
itrain = c(iris[1:30,],iris[51:80,],iris[101:130,])
itest = c(iris[31:40,],iris[81:90,],iris[131:140,])
ivalidate = c(iris[41:50,],iris[91:100,],iris[141:150,])
ilda = lda(itrain[,-5], grouping = itrain[,5])
iqda = qda(itrain[,-5], grouping = itrain[,5])
predict(ilda,itest[,-5])
predict(iqda,itest[,-5])
itrain = c(iris[1:30,],iris[51:80,],iris[101:130,])
itest = c(iris[31:40,],iris[81:90,],iris[131:140,])
ivalidate = c(iris[41:50,],iris[91:100,],iris[141:150,])
ilda = lda(itrain[,-5], grouping = itrain[,5])
iqda = qda(itrain[,-5], grouping = itrain[,5])
predict(ilda,itest[,-5])
predict(iqda,itest[,-5])
require('MASS')
itrain = c(iris[1:30,],iris[51:80,],iris[101:130,])
itest = c(iris[31:40,],iris[81:90,],iris[131:140,])
ivalidate = c(iris[41:50,],iris[91:100,],iris[141:150,])
ilda = lda(itrain[,-5], grouping = itrain[,5])
iqda = qda(itrain[,-5], grouping = itrain[,5])
predict(ilda,itest[,-5])
predict(iqda,itest[,-5])
itrain = c(iris[1:30,],iris[51:80,],iris[101:130,])
itest = c(iris[31:40,],iris[81:90,],iris[131:140,])
ivalidate = c(iris[41:50,],iris[91:100,],iris[141:150,])
ilda = lda(itrain[,-5], grouping = itrain[,5])
iqda = qda(itrain[,-5], grouping = itrain[,5])
predict(ilda,itest[,-5])
predict(iqda,itest[,-5])
itrain = c(iris[1:30,],iris[51:80,],iris[101:130,])
itest = c(iris[31:40,],iris[81:90,],iris[131:140,])
ivalidate = c(iris[41:50,],iris[91:100,],iris[141:150,])
ilda = lda(itrain[,c(1,2,3,4)], grouping = itrain[,5])
iqda = qda(itrain[,c(1,2,3,4)], grouping = itrain[,5])
predict(ilda,itest[,-5])
predict(iqda,itest[,-5])
?lda
itrain = c(iris[1:30,],iris[51:80,],iris[101:130,])
itest = c(iris[31:40,],iris[81:90,],iris[131:140,])
ivalidate = c(iris[41:50,],iris[91:100,],iris[141:150,])
ilda = lda(itrain[,c(1,2,3,4)], grouping = itrain[,5])
iqda = qda(itrain[,c(1,2,3,4)], grouping = itrain[,5])
predict(ilda,itest[,-5])
predict(iqda,itest[,-5])
itrain = c(iris[1:30,],iris[51:80,],iris[101:130,])
itrain
itrain = iris[c(1:30,51:80,101:130),]
itrain
itrain = iris[c(1:30,51:80,101:130),]
itest = iris[c(31:40,81:90,131:140),]
ivalidate = iris[c(41:50,91:100,141:150),]
ilda = lda(itrain[,c(1,2,3,4)], grouping = itrain[,5])
iqda = qda(itrain[,c(1,2,3,4)], grouping = itrain[,5])
predict(ilda,itest[,-5])
predict(iqda,itest[,-5])
require('MASS')
itrain = iris[c(1:30,51:80,101:130),]
itest = iris[c(31:40,81:90,131:140),]
ivalidate = iris[c(41:50,91:100,141:150),]
ilda = lda(itrain[,c(1,2,3,4)], grouping = itrain[,5])
iqda = qda(itrain[,c(1,2,3,4)], grouping = itrain[,5])
predict(ilda,itest[,-5])
predict(iqda,itest[,-5])
itrain = iris[c(1:30,51:80,101:130),]
itest = iris[c(31:40,81:90,131:140),]
ivalidate = iris[c(41:50,91:100,141:150),]
ilda = lda(itrain[,c(1,2,3,4)], grouping = itrain[,5])
iqda = qda(itrain[,c(1,2,3,4)], grouping = itrain[,5])
predict(ilda,itest[,-5])
predict(iqda,itest[,-5])
#LDA:
#QDA:
predict(ilda,ivalidate[,-5])
predict(iqda,ivalidate[,-5])
predict(ilda,ivalidate[,-5])
predict(iqda,ivalidate[,-5])
ivalidate
predict(ilda,itest[,-5])
predict(iqda,itest[,-5])
itest
?sd
?randIndex
require('modeltools')
require('flexclust')
?randIndex
prcomp(iris[,1:4],scale=TRUE)
summary(prcomp(iris[,1:4],scale=TRUE))
#2: prcomp
cov(iris[,1:4])
?isoMDS
#Kruskal's Non-Metric scaling
require("MASS")
?isoMDS
cl2=kmeans(faithful, centers = k)
#Check further
k=2
cl2=kmeans(faithful, centers = k)
#Alternatively
cl2$size
#Cluster centres
cl2$centers
summary(cl2)
#Cluster 1 has smaller eruptions and weighting times while cluster 2 has  larger
#Withing Sum of Squares
cl2$withinss
#Distance between cluster centers
dist(cl2$centers,method = "euclidian")
dist(cl10$centers, method = "euclidian")
load("olive.Rdata")
SSOlive = rep(0,10)
SSOlive[1] = (n-1)*sum(apply(olive,2,var))
for(i in 2:10)
{
SSOlive[i] = sum(kmeans(olive, centers=i)$withinss)
}
SSOlive
#k=10
cl10 = kmeans(olive, centers = 10)
ave = rep(0,10)
for(i in 1:10)
{
g1=olive[which(cl10$cluster==i),] #Selects datain first cluster
ng1=cl10$size[i] #records no of data points in cluster 1
total1=sum(as.matrix(dist(rbind(g1,cl10$centers[i,])))[ng1+1]) #Adds cluster center as additional row of cluster data, creates a distance matrix and sums them
ave[i]=total1 / ng1
}
ave
dist(cl10$centers, method = "euclidian")
setwd("C:/Users/Cormac/Downloads")
#Olive Oil Data
load("olive.Rdata")
SSOlive = rep(0,10)
SSOlive[1] = (n-1)*sum(apply(olive,2,var))
for(i in 2:10)
{
SSOlive[i] = sum(kmeans(olive, centers=i)$withinss)
}
SSOlive
#k=10
cl10 = kmeans(olive, centers = 10)
ave = rep(0,10)
for(i in 1:10)
{
g1=olive[which(cl10$cluster==i),] #Selects datain first cluster
ng1=cl10$size[i] #records no of data points in cluster 1
total1=sum(as.matrix(dist(rbind(g1,cl10$centers[i,])))[ng1+1]) #Adds cluster center as additional row of cluster data, creates a distance matrix and sums them
ave[i]=total1 / ng1
}
ave
dist(cl10$centers, method = "euclidian")
?sammon
require("MASS")
?sammon
shiny::runApp('~/GitHub/GitHubAPIVis')
source('~/GitHub/GitHubAPIVis/Test.R')
install.packages("shiny")
library(shiny); source('~/GitHub/GitHubAPIVis/Test.R')
source('~/GitHub/GitHubAPIVis/Test.R')
install.packages("shiny")
source('~/GitHub/GitHubAPIVis/Test.R')
source('~/GitHub/GitHubAPIVis/Test.R')
runApp('~/GitHub/GitHubAPIVis')
runApp('~/GitHub/GitHubAPIVis')
timeSeries = read.csv("P3timeseriesStudent.csv", header=FALSE)
timeSeries
tsdisplay(timeSeries,lag.max = 100)
timeSeriesSeasonal = ts(timeSeries[49:100,], frequency = 12)
ggseasonplot(timeSeriesSeasonal)
timeSeriesSeasonal
tsdisplay(timeSeriesSeasonal, lag.max = 50)
#d=1
tsdisplay(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(0,0,0))$residuals, lag.max=50)
Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(0,0,0))
#D=1
tsdisplay(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(0,1,0))$residuals, lag.max=50)
Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(0,1,0))
#P=1
tsdisplay(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(1,1,0))$residuals, lag.max=50)
Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(1,1,0))
#AIC = 192.55 BIC = 195.82
plot(forecast(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(1,1,0)), h=20))
forecast(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(1,1,0)), h=20)
require(forecast)
require(fma)
timeSeries = read.csv("P3timeseriesStudent.csv", header=FALSE)
timeSeries
tsdisplay(timeSeries,lag.max = 100)
timeSeriesSeasonal = ts(timeSeries[49:100,], frequency = 12)
ggseasonplot(timeSeriesSeasonal)
timeSeriesSeasonal
tsdisplay(timeSeriesSeasonal, lag.max = 50)
#d=1
tsdisplay(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(0,0,0))$residuals, lag.max=50)
Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(0,0,0))
#D=1
tsdisplay(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(0,1,0))$residuals, lag.max=50)
Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(0,1,0))
#P=1
tsdisplay(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(1,1,0))$residuals, lag.max=50)
Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(1,1,0))
#AIC = 192.55 BIC = 195.82
plot(forecast(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(1,1,0)), h=20))
forecast(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(1,1,0)), h=20)
#d=1
tsdisplay(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(0,0,0))$residuals, lag.max=50)
#d=1
tsdisplay(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(0,0,0))$residuals, lag.max=50)
par(mar=c(11,1,5,1))
#d=1
tsdisplay(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(0,0,0))$residuals, lag.max=50)
par(mar=c(11,1,1,1))
#d=1
tsdisplay(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(0,0,0))$residuals, lag.max=50)
par(mar=c(1,1,1,1))
#d=1
tsdisplay(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(0,0,0))$residuals, lag.max=50)
par(mar=c(1,2,3,1))
#d=1
tsdisplay(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(0,0,0))$residuals, lag.max=50)
#D=1
tsdisplay(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(0,1,0))$residuals, lag.max=50)
plot(forecast(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(1,1,0)), h=20))
forecast(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(1,1,0)), h=20)
timeSeries = read.csv("P3timeseriesStudent.csv", header=FALSE)
tsdisplay(timeSeries,lag.max = 100)
timeSeriesSeasonal = ts(timeSeries[50:100,], frequency = 12)
ggseasonplot(timeSeriesSeasonal)
tsdisplay(timeSeriesSeasonal, lag.max = 50)
#d=1
tsdisplay(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(0,0,0))$residuals, lag.max=50)
Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(0,0,0))
#D=1
tsdisplay(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(0,1,0))$residuals, lag.max=50)
Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(0,1,0))
#P=1
tsdisplay(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(1,1,0))$residuals, lag.max=50)
Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(1,1,0))
#AIC = 192.55 BIC = 195.82
plot(forecast(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(1,1,0)), h=20))
forecast(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(1,1,0)), h=20)
plot(forecast(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(1,1,0)), h=18))
forecast(Arima(timeSeriesSeasonal, order = c(0,1,0), seasonal = c(1,1,0)), h=18)
runApp('~/GitHub/GitHubAPIVis')
runApp('~/GitHub/GitHubAPIVis')
runApp('~/GitHub/GitHubAPIVis')
runApp('~/GitHub/GitHubAPIVis')
visFullData
order(visFullData$numberOfAppearances)
?order
visFullData[order(visFullData$numberOfAppearances),]
visFullData[order(visFullData$numberOfAppearances, decreasing = FALSE),]
runApp('~/GitHub/GitHubAPIVis')
runApp('~/GitHub/GitHubAPIVis')
runApp('~/GitHub/GitHubAPIVis')
runApp('~/GitHub/GitHubAPIVis')
runApp('~/GitHub/GitHubAPIVis')
runApp('~/GitHub/GitHubAPIVis')
runApp('~/GitHub/GitHubAPIVis')
runApp("app.R")
setwd("~/GitHub/GitHubAPIVis")
runApp("app.R")
#install.packages("shiny")
#library(shiny)
#runApp("app.R")
runGitHub( "GitHubAPIVis", "RoryMurphy1997")
shiny::runApp()
#Loads functions from R file to test
source("Interrogate API.R")
#Installs relevant packages only if they arent already installed
if (!require("pacman")) install.packages("pacman")
pacman::p_load("jsonlite", "httpuv", "httr")
#Opens necessary libraries
library(jsonlite)
library(httpuv)
library(httr)
oauth_endpoints("github")
#Stores relevant OAth information
myapp <- oauth_app(appname = "Software_Engineering_GitHub_Assignment",
key = "9cdd2af0684fe05044a7",
secret = "a58134728046024f716360e2b7bf4c97210e38fe")
# Get OAuth credentials
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
gtoken <- config(token = github_token)
#Use API to get repositories
req <- GET("https://api.github.com/repositories", gtoken)
stop_for_status(req)
json1 = content(req)
gitReposDF = jsonlite::fromJSON(jsonlite::toJSON(json1))
gitMicrosisRepo = gitReposDF[gitReposDF$node_id == "MDEwOlJlcG9zaXRvcnk0OA==",]
gitRubiniusRepo = gitReposDF[ gitReposDF$node_id == "MDEwOlJlcG9zaXRvcnkyNw==",]
gitJsawesomeRepo = gitReposDF[ gitReposDF$node_id == "MDEwOlJlcG9zaXRvcnkyOQ==",]
gitTwoRepos = rbind(gitMicrosisRepo, gitRubiniusRepo)
gitJSRepos = rbind(gitMicrosisRepo,gitJsawesomeRepo)
# Use API to find Microsis Languages
req <- GET("https://api.github.com/repos/Caged/microsis/languages", gtoken)
stop_for_status(req)
json1 = content(req)
gitMicrosisDF = jsonlite::fromJSON(jsonlite::toJSON(json1))
# Use API to find rubinius Languages
req <- GET("https://api.github.com/repos/rubinius/rubinius/languages", gtoken)
stop_for_status(req)
json1 = content(req)
gitRubiniusDF = jsonlite::fromJSON(jsonlite::toJSON(json1))
#Test PopularLanguages, the values have been cross checked manually using GitHub API on a web browser.
#1. Given an empty data frame, returns NULL and error message
PopularLanguages(NULL)
#2. Given a data frame with no languages_url value, returns null
DFTest = data.frame(Name = c("Sandra", "John"))
PopularLanguages(DFTest)
#3. Given a data frame of one repository, returns the most popular language use
#Should be Javascript (bytes = 124846)
PopularLanguages(gitMicrosisRepo)
#4. Given a data frame with more than one repository, returns the most popular language of each repository
#Should be JavaScript (bytes = 124846) and C (bytes = 23614738)
PopularLanguages(gitTwoRepos)
#Test ContributionsMade, the values have been cross checked manually using GitHub API on a web browser.
#1. Given an empty data frame, returns NULL and error message
ContributionsMade(NULL)
#2. Given a data frame with no contributors_url value, returns null
DFTest = data.frame(Name = c("Sandra", "John"))
ContributionsMade(DFTest)
#3. Given a data frame of one repository, returns the number of commits and number of contributors to this repository
#Should be 16 commits from 1 contributor
ContributionsMade(gitMicrosisRepo)
#4. Given a data frame with more than one repository, returns the number of commits and number of contributors to
#each repository
#Should be 16 commits and one contributor and  14599 commits by 30 contributors
ContributionsMade(gitTwoRepos)
#Test CreateDGraphData, the values have been cross checked manually using GitHub API on a web browser.
#1. Given an empty data frame, returns NULL and error message
CreateGraphData(NULL)
#2. Given a data frame with no Bytes value, returns null and error message
DFTest = data.frame(Language = c("Sandra", "John"))
CreateGraphData(DFTest)
#3. Given a data frame with no Language value, returns null and error message
DFTest = data.frame(NBytes = c("Sandra", "John"))
CreateGraphData(DFTest)
#4. Given data for one reository, returns relevant information for visualising said data.
#Should be 124846 totalBytes and 1 appearance for javascript
temp = cbind(ContributionsMade(gitMicrosisRepo),PopularLanguages(gitMicrosisRepo)[,-1])
CreateGraphData(temp)
#4. Given data for two reositories with different popular languages, returns relevant information for visualising said
#data.
#Should be 124846 totalBytes and 1 appearance for javascript and
#23614738 bytes and 1 apearance for C
temp2 = cbind(ContributionsMade(gitTwoRepos),PopularLanguages(gitTwoRepos)[,-1])
CreateGraphData(temp2)
#5. Given two repositories with same most pipular languages, combines number of bytes of language written.
#Should be 2 appearences and 251434 bytes of data
temp3 = cbind(ContributionsMade(gitJSRepos),PopularLanguages(gitJSRepos)[,-1])
temp3
CreateGraphData(temp3)
#Test CreateDGraphData, the values have been cross checked manually using GitHub API on a web browser.
#1. Given an empty data frame, returns NULL and error message
CreateGraphData(NULL)
#2. Given a data frame with no Bytes value, returns null and error message
DFTest = data.frame(Language = c("Sandra", "John"))
CreateGraphData(DFTest)
#3. Given a data frame with no Language value, returns null and error message
DFTest = data.frame(NBytes = c("Sandra", "John"))
CreateGraphData(DFTest)
#4. Given data for one reository, returns relevant information for visualising said data.
#Should be 124846 totalBytes and 1 appearance for javascript
temp = cbind(ContributionsMade(gitMicrosisRepo),PopularLanguages(gitMicrosisRepo)[,-1])
CreateGraphData(temp)
#5. Given data for two repositories with different popular languages, returns relevant information for visualising said
#data.
#Should be 124846 totalBytes and 1 appearance for javascript and
#23614738 bytes and 1 apearance for C
temp2 = cbind(ContributionsMade(gitTwoRepos),PopularLanguages(gitTwoRepos)[,-1])
CreateGraphData(temp2)
#6. Given two repositories with same most pipular languages, combines number of bytes of language written.
#Should be 2 appearences and 251434 bytes of data
temp3 = cbind(ContributionsMade(gitJSRepos),PopularLanguages(gitJSRepos)[,-1])
temp3
CreateGraphData(temp3)
#Test CreateDGraphData, the values have been cross checked manually using GitHub API on a web browser.
#1. Given an empty data frame, returns NULL and error message
CreateGraphData(NULL)
#2. Given a data frame with no Bytes value, returns null and error message
DFTest = data.frame(Language = c("Sandra", "John"))
CreateGraphData(DFTest)
#3. Given a data frame with no Language value, returns null and error message
DFTest = data.frame(NBytes = c("Sandra", "John"))
CreateGraphData(DFTest)
#4. Given data for one repository, returns relevant information for visualising said data.
#Should be 124846 totalBytes and 1 appearance for javascript
temp = cbind(ContributionsMade(gitMicrosisRepo),PopularLanguages(gitMicrosisRepo)[,-1])
CreateGraphData(temp)
#5. Given data for two repositories with different popular languages, returns relevant information for visualising said
#data.
#Should be 124846 totalBytes and 1 appearance for javascript and
#23614738 bytes and 1 apearance for C
temp2 = cbind(ContributionsMade(gitTwoRepos),PopularLanguages(gitTwoRepos)[,-1])
CreateGraphData(temp2)
#6. Given two repositories with same most pipular languages, combines number of bytes of language written.
#Should be 2 appearences and 251434 bytes of data
temp3 = cbind(ContributionsMade(gitJSRepos),PopularLanguages(gitJSRepos)[,-1])
temp3
CreateGraphData(temp3)
